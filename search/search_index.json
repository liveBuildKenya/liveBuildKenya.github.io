{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction It works! Why? The journey to master software development; Bumpy, no doubt. So, what is a software? Software A software is a computer program and other operating information used by a computer. A good software should deliver the main required functionality. It should also: Be written in a way that can be adapted to changes Respond in a quick time Make efficient use of resources such as memory and processor time Be acceptable by the user it is built for. Now, what is software engineering? Software Engineering Software engineering is defined as a process of analyzing user requirements and then designing, building and testing software application which will satisfy those requirements. It is the application of a systematic, disciplined, computable approach for the development, operation and maintenance of software.","title":"Introduction"},{"location":"#introduction","text":"It works! Why? The journey to master software development; Bumpy, no doubt. So, what is a software?","title":"Introduction"},{"location":"#software","text":"A software is a computer program and other operating information used by a computer. A good software should deliver the main required functionality. It should also: Be written in a way that can be adapted to changes Respond in a quick time Make efficient use of resources such as memory and processor time Be acceptable by the user it is built for. Now, what is software engineering?","title":"Software"},{"location":"#software-engineering","text":"Software engineering is defined as a process of analyzing user requirements and then designing, building and testing software application which will satisfy those requirements. It is the application of a systematic, disciplined, computable approach for the development, operation and maintenance of software.","title":"Software Engineering"},{"location":"database-management/postgres/installation/","text":"Install on linux Ubuntu Use the PostgreSQL Apt Repository. This will integrate with your normal systems and patch management, and provide automatic updates for all supported version of PostgreSQK throughout the support lifetime of PostgreSQL. To use the apt reporitory follow hese steps: Create the file repository configuration sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' Import the repository signing key wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - Update the package lists sudo apt-get update Install the latest version of postgreSQL sudo apt-get -y install postgresql Note: If you want a specific version, use 'postgresql-12' or similar instead of 'postgresql':","title":"Installation"},{"location":"database-management/postgres/installation/#install-on-linux","text":"","title":"Install on linux"},{"location":"database-management/postgres/installation/#ubuntu","text":"Use the PostgreSQL Apt Repository. This will integrate with your normal systems and patch management, and provide automatic updates for all supported version of PostgreSQK throughout the support lifetime of PostgreSQL. To use the apt reporitory follow hese steps: Create the file repository configuration sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' Import the repository signing key wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - Update the package lists sudo apt-get update Install the latest version of postgreSQL sudo apt-get -y install postgresql Note: If you want a specific version, use 'postgresql-12' or similar instead of 'postgresql':","title":"Ubuntu"},{"location":"server-management/introduction/","text":"Proxmox VE or VMware vShpere: Which is best? In the space of containerisation and virtualisation, it may not be obvious to identify the software tool of trade. However, there are two standout packages that most businesses opt for; Proxmox VE and VMware vShpere. What is Proxmox? Proxmox, developed by Proxmox Server Solutions in Austria, is a complete open source server management platform for enterprise virtualisation released under the GNU General Public License. It is a Debian-based Linux distribution with a modified Ubuntu LTS kernel enabling the deployment and management of VMs and containers, such as KVM (Kernel-based VM) for VMs and Linux Containers (LXC) for containers. The software also includes a bare-metal installer, web-based management interface and many command-line tools. There is also a REST API to support third-party tools. Proxmox can be clustered across multiple server nodes for high availability. When deployed, the resource manager called Proxmox VE HA Manager monitors all VMs and containers on the whole cluster and automatically gets into action if one of them fails. There is also an integrated live/online migration feature, this enables the movement of VMs from one Proxmox VE cluster node to another without any downtime. The process can be initiated by administrators with either scripts or the web interface. The Proxmox Virtual Environment supports a maximum of 12TB of RAM and 768 logical CPUs per host. It also supports Intel EMT64 or AMD64 with Intel VT/AMD-V CPU flag. It also features a built-in firewall that is customisable allowing configurations via GUI or CLI. Firewall rules can be set up for all hosts inside a cluster or define rules for VMs and containers only. What is VMware vSphere? vSphere is the most popular virtualisation software in VMware's portfolio and is now in its seventh iteration. It was originally launched in 2009 as 'VMware Infrastructure', but has changed substantially over the last 12 years. The product includes hypervisor software and a management platform. The Type 1 hypervisor, also known as ESXi, is a bare-metal version that includes the OS kernel. It has vCentre server management system, previously known as VirtualCenter, that offers a centralised view across all ESXi hosts. The hypervisor is similar to your average operating system in that it is directly installed into the physical hardware. This way, customers can create multiple virtual machines (VMs) and run systems like Windows, Linux, macOS, Solaris, and more on a single device. This means there is a layer of storage handled by virtualisation. Another bonus of ESXi is it can run on Intel processors (Xeon and up) and AMD Opteron and Epyc processors -- this crosses both 32-bit and 64-bit guest operating systems, although 32-bit processors aren\u2019t supported. ESXi uses a 64-bit VMkernel. The hypervisor, which can be installed on a hard disk, USB device, or SD card, can support the following resources per host: 4,096 virtual processors, 512 VMs, 4TB of RAM and 320 logical CPUs. VMware\u2019s ESXi is available as a free download or as part of a paid package. Naturally, the free version provides only limited functionality and can\u2019t be managed by Center (see below). vSphere is currently on its 7th iteration, first announced in March 2020, and is the first version to feature vSphere with Kubernetes, formerly known as Project Pacific. VMware vCenter is a software suite that manages the whole of the VMware virtualisation infrastructure, acting as a single window. From here, the assignment of VMs to hosts is managed, as well as the assignment of resources to tasks, based on policies set by the administrator. A single instance of vCenter can manage up to 1,000 hosts at a time, across up to 10,000 active VMs or 15,000 registered VMs It also enables the use of features such as vSphere Distributed Resource Scheduler (DRS), vSphere High Availability (HA), vSphere vMotion, and vSphere Storage vMotion. It also provides the API for vSphere and manages ESXi. It can be installed on a supported version of Windows or used as a preconfigured Linux version known as vCenter Server Appliance. vCenter Server also permits Host Profiles, allowing users to define rules for specific ESXi hosts. With the latest version of its virtualisation software - vSphere 7 - VMware has added full integration with Kubernetes, which it's touting as the \"biggest vSphere innovation since the launch of the ESXi hypervisor\". This means administrators can provision, run, and manage Kubernetes clusters on top of vSphere via the Kubernetes interface. Supporting both containers and VMs on a single platform allows vSphere 7 to run Kubernetes pods on VMs using the vSphere POD Service. VMware vSphere PODs can be managed like existing VMs. VMware VSphere vs Proxmox VE ESXi is a mostly closed off, proprietary product that has a free version with limited features. However, enterprise features are not available in the free version. Proxmox is a free, open source product based on other free, open source products (KVM, LXC, etc) with all features enabled. VMware vSphere has more features overall than Proxmox, although Proxmox's features are more useful. Proxmox can automatically enable nodes to use the same shared storage when the user adds them to a cluster. While ESXi obliges the user to manually configure a node to use the shared storage from its cluster. While both technologies are used for cloud computing and server consolidation, the typical usage profile of Proxmox is in virtualised server isolation and software development. VMware vSphere is more likely to be used for business-critical applications and infrastructure as a service (IaaS). ESXi also uses proprietary technology to support virtualisation (VT-x for Intel processors and AMD-V for AMD processors). Compare this with the situation with Proxmox; its KVM uses generic x86 virtualisation technology.","title":"Introduction"},{"location":"server-management/introduction/#proxmox-ve-or-vmware-vshpere-which-is-best","text":"In the space of containerisation and virtualisation, it may not be obvious to identify the software tool of trade. However, there are two standout packages that most businesses opt for; Proxmox VE and VMware vShpere.","title":"Proxmox VE or VMware vShpere: Which is best?"},{"location":"server-management/introduction/#what-is-proxmox","text":"Proxmox, developed by Proxmox Server Solutions in Austria, is a complete open source server management platform for enterprise virtualisation released under the GNU General Public License. It is a Debian-based Linux distribution with a modified Ubuntu LTS kernel enabling the deployment and management of VMs and containers, such as KVM (Kernel-based VM) for VMs and Linux Containers (LXC) for containers. The software also includes a bare-metal installer, web-based management interface and many command-line tools. There is also a REST API to support third-party tools. Proxmox can be clustered across multiple server nodes for high availability. When deployed, the resource manager called Proxmox VE HA Manager monitors all VMs and containers on the whole cluster and automatically gets into action if one of them fails. There is also an integrated live/online migration feature, this enables the movement of VMs from one Proxmox VE cluster node to another without any downtime. The process can be initiated by administrators with either scripts or the web interface. The Proxmox Virtual Environment supports a maximum of 12TB of RAM and 768 logical CPUs per host. It also supports Intel EMT64 or AMD64 with Intel VT/AMD-V CPU flag. It also features a built-in firewall that is customisable allowing configurations via GUI or CLI. Firewall rules can be set up for all hosts inside a cluster or define rules for VMs and containers only.","title":"What is Proxmox?"},{"location":"server-management/introduction/#what-is-vmware-vsphere","text":"vSphere is the most popular virtualisation software in VMware's portfolio and is now in its seventh iteration. It was originally launched in 2009 as 'VMware Infrastructure', but has changed substantially over the last 12 years. The product includes hypervisor software and a management platform. The Type 1 hypervisor, also known as ESXi, is a bare-metal version that includes the OS kernel. It has vCentre server management system, previously known as VirtualCenter, that offers a centralised view across all ESXi hosts. The hypervisor is similar to your average operating system in that it is directly installed into the physical hardware. This way, customers can create multiple virtual machines (VMs) and run systems like Windows, Linux, macOS, Solaris, and more on a single device. This means there is a layer of storage handled by virtualisation. Another bonus of ESXi is it can run on Intel processors (Xeon and up) and AMD Opteron and Epyc processors -- this crosses both 32-bit and 64-bit guest operating systems, although 32-bit processors aren\u2019t supported. ESXi uses a 64-bit VMkernel. The hypervisor, which can be installed on a hard disk, USB device, or SD card, can support the following resources per host: 4,096 virtual processors, 512 VMs, 4TB of RAM and 320 logical CPUs. VMware\u2019s ESXi is available as a free download or as part of a paid package. Naturally, the free version provides only limited functionality and can\u2019t be managed by Center (see below). vSphere is currently on its 7th iteration, first announced in March 2020, and is the first version to feature vSphere with Kubernetes, formerly known as Project Pacific. VMware vCenter is a software suite that manages the whole of the VMware virtualisation infrastructure, acting as a single window. From here, the assignment of VMs to hosts is managed, as well as the assignment of resources to tasks, based on policies set by the administrator. A single instance of vCenter can manage up to 1,000 hosts at a time, across up to 10,000 active VMs or 15,000 registered VMs It also enables the use of features such as vSphere Distributed Resource Scheduler (DRS), vSphere High Availability (HA), vSphere vMotion, and vSphere Storage vMotion. It also provides the API for vSphere and manages ESXi. It can be installed on a supported version of Windows or used as a preconfigured Linux version known as vCenter Server Appliance. vCenter Server also permits Host Profiles, allowing users to define rules for specific ESXi hosts. With the latest version of its virtualisation software - vSphere 7 - VMware has added full integration with Kubernetes, which it's touting as the \"biggest vSphere innovation since the launch of the ESXi hypervisor\". This means administrators can provision, run, and manage Kubernetes clusters on top of vSphere via the Kubernetes interface. Supporting both containers and VMs on a single platform allows vSphere 7 to run Kubernetes pods on VMs using the vSphere POD Service. VMware vSphere PODs can be managed like existing VMs.","title":"What is VMware vSphere?"},{"location":"server-management/introduction/#vmware-vsphere-vs-proxmox-ve","text":"ESXi is a mostly closed off, proprietary product that has a free version with limited features. However, enterprise features are not available in the free version. Proxmox is a free, open source product based on other free, open source products (KVM, LXC, etc) with all features enabled. VMware vSphere has more features overall than Proxmox, although Proxmox's features are more useful. Proxmox can automatically enable nodes to use the same shared storage when the user adds them to a cluster. While ESXi obliges the user to manually configure a node to use the shared storage from its cluster. While both technologies are used for cloud computing and server consolidation, the typical usage profile of Proxmox is in virtualised server isolation and software development. VMware vSphere is more likely to be used for business-critical applications and infrastructure as a service (IaaS). ESXi also uses proprietary technology to support virtualisation (VT-x for Intel processors and AMD-V for AMD processors). Compare this with the situation with Proxmox; its KVM uses generic x86 virtualisation technology.","title":"VMware VSphere vs Proxmox VE"},{"location":"server-management/kubernetes/image-deployment/","text":"","title":"Image Deployment"},{"location":"server-management/kubernetes/setup/","text":"Managing Kubernetes on Ubuntu 20.04 Server Kubernetes (k8s) is a free and open-source container orchestration tool. It is used for deploying, scaling and managing containerized based applications. In this article we will demonstrate how to install a Kubernetes cluster using kubeadm . Before you begin ensure you have the following: - One or more machines running a deb/rpm-compatible Linux OS; for example: Ubuntu or CentOS. - 2 GiB or more of RAM per machine--any less leaves little room for your apps. - At least 2 CPUs on the machine that you use as a control-plane node. - Full network connectivity among all machines in the cluster. You can use either a public or a private network. - Privileged user with sudo rights Lab Setup The foloowing are the details of my lab setup: Machine 1 (Ubuntu 20.04 LTS Server) \u2013 kubemaster.com \u2013 192.168.1.4 Machine 2 (Ubuntu 20.04 LTS Server) \u2013 kubeworkernode1.com \u2013 192.168.1.5 Machine 3 (Ubuntu 20.04 LTS Server) \u2013 kubeworkernode2.com \u2013 192.168.1.6 Step 1 - Set the hostname and add entries on the /etc/hosts file On each of the machine set the hostname on each node. master node: sudo hostnamectl set-hostname \"kubemaster.com\" worker node 1: sudo hostnamectl set-hostname \"kubeworkernode1.com\" worker node 2: sudo hostnamectl set-hostname \"kubeworkernode2.com\" After the above commands, add the following entries in the /etc/hosts files on each node, 192.168.1.4 kubemaster.com 192.168.1.5 kubeworkernode1.com 192.168.1.6 kubeworkernode2.com Step 2 Install a Container runtime on all 3 nodes For this set up we will install docker. Uninstall old version of docker. sudo apt-get remove docker docker-engine docker.io containerd runc It\u2019s OK if apt-get reports that none of these packages are installed. The contents of /var/lib/docker/, including images, containers, volumes, and networks, are preserved. Set up the repository Update the apt package index and install packages to allow apt to use a repository over HTTPS: sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker\u2019s official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Use the following command to set up the stable repository. To add the nightly or test repository, add the word nightly or test (or both) after the word stable in the commands below. echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install Docker Engine Update the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io To install a specific version of Docker Engine, list the available versions in the repo, then select and install: List the versions available in your repo: apt-cache madison docker-ce Install a specific version using the version string from the second column sudo apt-get install docker-ce=<VERSION_STRING> docker-ce-cli=<VERSION_STRING> containerd.io Now start and enable docker service on each node using beneath systemctl command sudo systemctl enable docker.service --now Run the following command to verify the status of docker service and its version, systemctl status docker docker --version Step 3 Disable swap and enable IP forwarding on all nodes To disable swap, edit /etc/fstab file and comment out the line which includes entry either swap partition or swap file. sudo vi /etc/fstab Save & exit the file Run swapoff command to disable the swap on the fly sudo swapoff -a To enable the ip forwarding permanently, edit the file \u201c/etc/sysctl.conf\u201d and look for line \u201cnet.ipv4.ip_forward=1\u201d and un-comment it. After making the changes in the file, execute the following command sudo sysctl -p Step 4 -Install Kubectl, kubelet and kubeadm on all nodes Update the apt package index and install packages needed to use the Kubernetes apt repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl Download the Google Cloud public signing key: sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg Add the Kubernetes apt repository: echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list Update apt package index, install kubelet, kubeadm and kubectl, and pin their version: sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl Step 5 - Initialize Kubernetes Cluster using kubeadm (from master node) Login to your master node kubemaster.com and run below \u2018kubeadm init\u2018 command to initialize Kubernetes cluster, sudo kubeadm init --control-plane-endpoint=kubemaster.com Once the cluster is initialized successfully, follow the instruction on the output Now Join the worker nodes kubeworkernode1.com and kubeworkernode2.com to cluster, command to join the cluster is already there in the output. Copy \u201ckubeadm join\u201d command and paste it on both nodes (worker nodes). From the master node run \u201ckubectl get nodes\u201d command to verify nodes status. As we can see both worker nodes and master node have joined the cluster, but status of each node is \u201cNotReady\u201d. To make the status \u201cReady\u201d we must deploy Container Network Interface (CNI) based Pod network add-ons like calico, kube-router and weave-net. As the name suggests, pod network add-ons allow pods to communicate each other. Step 6 - Deploy Calico Pod Network Add-on (Master Node) From the master node, run the following command to install Calico pod network add-on, kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml Once it has been deployed successfully then nodes status will become ready, let\u2019s re-run kubectl command to verify nodes status Confirm and veryfy the status of pods from all namespaces kubectl get pods --all-namespaces Note: To enable bash completion feature on your master node, execute the followings echo 'source <(kubectl completion bash)' >>~/.bashrc source .bashrc","title":"Setup"},{"location":"server-management/kubernetes/setup/#managing-kubernetes-on-ubuntu-2004-server","text":"Kubernetes (k8s) is a free and open-source container orchestration tool. It is used for deploying, scaling and managing containerized based applications. In this article we will demonstrate how to install a Kubernetes cluster using kubeadm . Before you begin ensure you have the following: - One or more machines running a deb/rpm-compatible Linux OS; for example: Ubuntu or CentOS. - 2 GiB or more of RAM per machine--any less leaves little room for your apps. - At least 2 CPUs on the machine that you use as a control-plane node. - Full network connectivity among all machines in the cluster. You can use either a public or a private network. - Privileged user with sudo rights","title":"Managing Kubernetes on Ubuntu 20.04 Server"},{"location":"server-management/kubernetes/setup/#lab-setup","text":"The foloowing are the details of my lab setup: Machine 1 (Ubuntu 20.04 LTS Server) \u2013 kubemaster.com \u2013 192.168.1.4 Machine 2 (Ubuntu 20.04 LTS Server) \u2013 kubeworkernode1.com \u2013 192.168.1.5 Machine 3 (Ubuntu 20.04 LTS Server) \u2013 kubeworkernode2.com \u2013 192.168.1.6","title":"Lab Setup"},{"location":"server-management/kubernetes/setup/#step-1-set-the-hostname-and-add-entries-on-the-etchosts-file","text":"On each of the machine set the hostname on each node. master node: sudo hostnamectl set-hostname \"kubemaster.com\" worker node 1: sudo hostnamectl set-hostname \"kubeworkernode1.com\" worker node 2: sudo hostnamectl set-hostname \"kubeworkernode2.com\" After the above commands, add the following entries in the /etc/hosts files on each node, 192.168.1.4 kubemaster.com 192.168.1.5 kubeworkernode1.com 192.168.1.6 kubeworkernode2.com","title":"Step 1 - Set the hostname and add entries on the /etc/hosts file"},{"location":"server-management/kubernetes/setup/#step-2-install-a-container-runtime-on-all-3-nodes","text":"For this set up we will install docker.","title":"Step 2 Install a Container runtime on all 3 nodes"},{"location":"server-management/kubernetes/setup/#uninstall-old-version-of-docker","text":"sudo apt-get remove docker docker-engine docker.io containerd runc It\u2019s OK if apt-get reports that none of these packages are installed. The contents of /var/lib/docker/, including images, containers, volumes, and networks, are preserved.","title":"Uninstall old version of docker."},{"location":"server-management/kubernetes/setup/#set-up-the-repository","text":"Update the apt package index and install packages to allow apt to use a repository over HTTPS: sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker\u2019s official GPG key: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Use the following command to set up the stable repository. To add the nightly or test repository, add the word nightly or test (or both) after the word stable in the commands below. echo \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null","title":"Set up the repository"},{"location":"server-management/kubernetes/setup/#install-docker-engine","text":"Update the apt package index, and install the latest version of Docker Engine and containerd, or go to the next step to install a specific version: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io To install a specific version of Docker Engine, list the available versions in the repo, then select and install: List the versions available in your repo: apt-cache madison docker-ce Install a specific version using the version string from the second column sudo apt-get install docker-ce=<VERSION_STRING> docker-ce-cli=<VERSION_STRING> containerd.io Now start and enable docker service on each node using beneath systemctl command sudo systemctl enable docker.service --now Run the following command to verify the status of docker service and its version, systemctl status docker docker --version","title":"Install Docker Engine"},{"location":"server-management/kubernetes/setup/#step-3-disable-swap-and-enable-ip-forwarding-on-all-nodes","text":"To disable swap, edit /etc/fstab file and comment out the line which includes entry either swap partition or swap file. sudo vi /etc/fstab Save & exit the file Run swapoff command to disable the swap on the fly sudo swapoff -a To enable the ip forwarding permanently, edit the file \u201c/etc/sysctl.conf\u201d and look for line \u201cnet.ipv4.ip_forward=1\u201d and un-comment it. After making the changes in the file, execute the following command sudo sysctl -p","title":"Step 3 Disable swap and enable IP forwarding on all nodes"},{"location":"server-management/kubernetes/setup/#step-4-install-kubectl-kubelet-and-kubeadm-on-all-nodes","text":"Update the apt package index and install packages needed to use the Kubernetes apt repository sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl Download the Google Cloud public signing key: sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg Add the Kubernetes apt repository: echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list Update apt package index, install kubelet, kubeadm and kubectl, and pin their version: sudo apt-get update sudo apt-get install -y kubelet kubeadm kubectl","title":"Step 4 -Install Kubectl, kubelet and kubeadm on all nodes"},{"location":"server-management/kubernetes/setup/#step-5-initialize-kubernetes-cluster-using-kubeadm-from-master-node","text":"Login to your master node kubemaster.com and run below \u2018kubeadm init\u2018 command to initialize Kubernetes cluster, sudo kubeadm init --control-plane-endpoint=kubemaster.com Once the cluster is initialized successfully, follow the instruction on the output Now Join the worker nodes kubeworkernode1.com and kubeworkernode2.com to cluster, command to join the cluster is already there in the output. Copy \u201ckubeadm join\u201d command and paste it on both nodes (worker nodes). From the master node run \u201ckubectl get nodes\u201d command to verify nodes status. As we can see both worker nodes and master node have joined the cluster, but status of each node is \u201cNotReady\u201d. To make the status \u201cReady\u201d we must deploy Container Network Interface (CNI) based Pod network add-ons like calico, kube-router and weave-net. As the name suggests, pod network add-ons allow pods to communicate each other.","title":"Step 5 - Initialize Kubernetes Cluster using kubeadm (from master node)"},{"location":"server-management/kubernetes/setup/#step-6-deploy-calico-pod-network-add-on-master-node","text":"From the master node, run the following command to install Calico pod network add-on, kubectl apply -f https://docs.projectcalico.org/v3.14/manifests/calico.yaml Once it has been deployed successfully then nodes status will become ready, let\u2019s re-run kubectl command to verify nodes status Confirm and veryfy the status of pods from all namespaces kubectl get pods --all-namespaces Note: To enable bash completion feature on your master node, execute the followings echo 'source <(kubectl completion bash)' >>~/.bashrc source .bashrc","title":"Step 6 - Deploy Calico Pod Network Add-on (Master Node)"},{"location":"server-management/security/ddos/","text":"DDoS attack A distributed denial-of-service (DDoS) attack is a malicious attempt to disrupt legitimate traffic of a targeted server, service or network, by bombarding it with so much traffic from multiple machines. Typically, the attacker tries to saturate a system with so many connections and requests until the server it is no longer able to accept new traffic, or becomes so slow that it is effectively unusable. DDoS attacks can cost an organization both time and money while their resources and services are inaccessible. Luckily, servers can be configured to fight back. How does a DDoS attack work? A DDoS attack requires an attacker to gain control of a network of online machines in order to carry out an attack. Usually computers and other devices are infected with malware, turning each one into a bot. The attacker then has remote control over the group of bots, which is called a botnet. Once in control, an attacker can command their botnet to conduct DDoS on a target. In this case, the infected devices are also victims of the attack. When the IP address of a victim is targeted by the botnet, each bot will respond by sending a large number of requests or very large requests to the target, potentially causing the targeted server or network to overflow capacity, resulting in a denial-of-service to normal traffic. Because each bot is a legitimate Internet device, separating the attack traffic from normal traffic can be difficult. How do you know if an attack is happening? Symptoms of a DDoS attack can resemble non-malicious availability issues, such as technical problems with a particular network or a system administrator performing maintenance. However, the following symptoms could indicate a DoS or DDoS attack: Unusually slow network performance Unavailability of a particular website Unable to access any website The best way to detect and identify a DDoS attack would be via network traffic monitoring and analysis. Network traffic can be monitored via a firewall, intrusion detection system and system or application logs. Using NGINX to prevent DDoS Attacks Nginx, a highly popular server system for Unix machines, comes with enough built-in functionality to greatly limit the effectiveness of DDoS attacks. These features could handle a DDoS attack by regulating the incoming traffic and by controlling the traffic as it is proxied to backend services. Nginx worker connections One of the important parameters that we tweak is the number of worker process and number of worker connections in the Nginx configuration file /etc/nginx/nginx.conf. events { worker_connections 50000; } This setting allows each of the worker process to handle up to 50000 connections. Limiting requests rate Among the many useful tactics worth trying for staving off DDoS attacks, one of the simplest and most effective is the limiting of incoming traffic rates. You can limit the rate at which NGINX accept incoming requests to a normal value for your service from a particular client IP address within a certain period. limit_req_zone $binary_remote_addr zone=one:1m rate=30r/m; server { location /wp_login.php { limit_req zone=one; } } This example creates a memory area called one, that can hold up to 16,000 (1m) unique IPs, and the 30r/m means that only 30 requests per minute are allowed. Then, we use the limit_req directive to rate limit the connections to a particular location or file, in this case wp_login.php. Limiting number of connections You can limit the number of connections that can be opened by a single client IP address limit_conn_zone $binary_remote_addr zone=two:1m; server { location / { limit_conn two 10; } } This example creates a memory zone called two to store requests for the specified key, in this case the client IP address, $binary_remote_addr. Then the limit_conn directive sets a maximum of 10 connections from each client IP address. Timeout parameters Slow connections can represent an attempt to keep connections open for a long time. As a result, the server can\u2019t accept new connections. server { client_body_timeout 5s; client_header_timeout 5s; } In this example, the client_body_timeout directive defines how long Nginx is waiting between the writes of the client body and client_header_timeout means how long Nginx is waiting between the writes of client header. Both are set to 5 seconds. Limit requests size Similarly, large buffer values or large HTTP requests size make DDoS attacks easier. So, we limit the following buffer values in the Nginx configuration file to mitigate DDoS attacks. client_body_buffer_size 200K; client_header_buffer_size 2k; client_max_body_size 200k; large_client_header_buffers 3 1k; Blacklist IP adresses If you can identify the client IP addresses being used for an attack, you can blacklist them with the deny directive so that NGINX and NGINX Plus do not accept their connections or requests. location / { deny 123.123.123.3; deny 123.123.123.0/24; } Enable sysctl based protection Additionally we can tweak kernel and system variables in our server. Edit the file /etc/sysctl.conf, and set these two lines to 1 like this: net.ipv4.conf.all.rp_filter = 1 net.ipv4.tcp_syncookies = 1 The first parameter enables protection against IP spoofing, and the second allows TCP SYN cookie protection. Nginx as load balancer When Nginx is used as load balancer, it is possible to adjust parameters to limit the connections number for each server: upstream domain { server 123.123.123.1:80 max_conns=100; server 123.123.123.2:80 max_conns=100; queue 20 timeout=10s; } Here the max_conns directive specifies the number of connections Nginx can open up for the server. The queue directive limits the number of requests that have been queued when all servers in this group have reached the connection limit. Finally, the timeout directive specifies how long a request can be retained in the queue.","title":"DDOS"},{"location":"server-management/security/ddos/#ddos-attack","text":"A distributed denial-of-service (DDoS) attack is a malicious attempt to disrupt legitimate traffic of a targeted server, service or network, by bombarding it with so much traffic from multiple machines. Typically, the attacker tries to saturate a system with so many connections and requests until the server it is no longer able to accept new traffic, or becomes so slow that it is effectively unusable. DDoS attacks can cost an organization both time and money while their resources and services are inaccessible. Luckily, servers can be configured to fight back.","title":"DDoS attack"},{"location":"server-management/security/ddos/#how-does-a-ddos-attack-work","text":"A DDoS attack requires an attacker to gain control of a network of online machines in order to carry out an attack. Usually computers and other devices are infected with malware, turning each one into a bot. The attacker then has remote control over the group of bots, which is called a botnet. Once in control, an attacker can command their botnet to conduct DDoS on a target. In this case, the infected devices are also victims of the attack. When the IP address of a victim is targeted by the botnet, each bot will respond by sending a large number of requests or very large requests to the target, potentially causing the targeted server or network to overflow capacity, resulting in a denial-of-service to normal traffic. Because each bot is a legitimate Internet device, separating the attack traffic from normal traffic can be difficult.","title":"How does a DDoS attack work?"},{"location":"server-management/security/ddos/#how-do-you-know-if-an-attack-is-happening","text":"Symptoms of a DDoS attack can resemble non-malicious availability issues, such as technical problems with a particular network or a system administrator performing maintenance. However, the following symptoms could indicate a DoS or DDoS attack: Unusually slow network performance Unavailability of a particular website Unable to access any website The best way to detect and identify a DDoS attack would be via network traffic monitoring and analysis. Network traffic can be monitored via a firewall, intrusion detection system and system or application logs.","title":"How do you know if an attack is happening?"},{"location":"server-management/security/ddos/#using-nginx-to-prevent-ddos-attacks","text":"Nginx, a highly popular server system for Unix machines, comes with enough built-in functionality to greatly limit the effectiveness of DDoS attacks. These features could handle a DDoS attack by regulating the incoming traffic and by controlling the traffic as it is proxied to backend services.","title":"Using NGINX to prevent DDoS Attacks"},{"location":"server-management/security/ddos/#nginx-worker-connections","text":"One of the important parameters that we tweak is the number of worker process and number of worker connections in the Nginx configuration file /etc/nginx/nginx.conf. events { worker_connections 50000; } This setting allows each of the worker process to handle up to 50000 connections.","title":"Nginx worker connections"},{"location":"server-management/security/ddos/#limiting-requests-rate","text":"Among the many useful tactics worth trying for staving off DDoS attacks, one of the simplest and most effective is the limiting of incoming traffic rates. You can limit the rate at which NGINX accept incoming requests to a normal value for your service from a particular client IP address within a certain period. limit_req_zone $binary_remote_addr zone=one:1m rate=30r/m; server { location /wp_login.php { limit_req zone=one; } } This example creates a memory area called one, that can hold up to 16,000 (1m) unique IPs, and the 30r/m means that only 30 requests per minute are allowed. Then, we use the limit_req directive to rate limit the connections to a particular location or file, in this case wp_login.php.","title":"Limiting requests rate"},{"location":"server-management/security/ddos/#limiting-number-of-connections","text":"You can limit the number of connections that can be opened by a single client IP address limit_conn_zone $binary_remote_addr zone=two:1m; server { location / { limit_conn two 10; } } This example creates a memory zone called two to store requests for the specified key, in this case the client IP address, $binary_remote_addr. Then the limit_conn directive sets a maximum of 10 connections from each client IP address.","title":"Limiting number of connections"},{"location":"server-management/security/ddos/#timeout-parameters","text":"Slow connections can represent an attempt to keep connections open for a long time. As a result, the server can\u2019t accept new connections. server { client_body_timeout 5s; client_header_timeout 5s; } In this example, the client_body_timeout directive defines how long Nginx is waiting between the writes of the client body and client_header_timeout means how long Nginx is waiting between the writes of client header. Both are set to 5 seconds.","title":"Timeout parameters"},{"location":"server-management/security/ddos/#limit-requests-size","text":"Similarly, large buffer values or large HTTP requests size make DDoS attacks easier. So, we limit the following buffer values in the Nginx configuration file to mitigate DDoS attacks. client_body_buffer_size 200K; client_header_buffer_size 2k; client_max_body_size 200k; large_client_header_buffers 3 1k;","title":"Limit requests size"},{"location":"server-management/security/ddos/#blacklist-ip-adresses","text":"If you can identify the client IP addresses being used for an attack, you can blacklist them with the deny directive so that NGINX and NGINX Plus do not accept their connections or requests. location / { deny 123.123.123.3; deny 123.123.123.0/24; }","title":"Blacklist IP adresses"},{"location":"server-management/security/ddos/#enable-sysctl-based-protection","text":"Additionally we can tweak kernel and system variables in our server. Edit the file /etc/sysctl.conf, and set these two lines to 1 like this: net.ipv4.conf.all.rp_filter = 1 net.ipv4.tcp_syncookies = 1 The first parameter enables protection against IP spoofing, and the second allows TCP SYN cookie protection.","title":"Enable sysctl based protection"},{"location":"server-management/security/ddos/#nginx-as-load-balancer","text":"When Nginx is used as load balancer, it is possible to adjust parameters to limit the connections number for each server: upstream domain { server 123.123.123.1:80 max_conns=100; server 123.123.123.2:80 max_conns=100; queue 20 timeout=10s; } Here the max_conns directive specifies the number of connections Nginx can open up for the server. The queue directive limits the number of requests that have been queued when all servers in this group have reached the connection limit. Finally, the timeout directive specifies how long a request can be retained in the queue.","title":"Nginx as load balancer"},{"location":"server-management/security/secure-linux-server/","text":"Secure Linux Server From Hackers Every server is vulnerable from hackers especially if you have not taken any action to prevent this from happening. So what can you do to protect yourself from a hacker? Enable Automatic Updates Most servers are hacked because they did not have any security updates. To update manually you first check your repositories for updates by running the following command as the root user. apt update To upgrade you run the command apt upgrade To install updates automatically you run: apt install unattended-upgrades to setup automatic upgrades you run: dpkg-reconfigure --priority=low unattended-upgrades Limited User Account The root user has the ability to make damaging changes to your linux system. So lets limit the user account. As the root user run the following command: adduser <username> Then add user to the sudo group usermod -aG sudo <username> SSH Keys We will create an authentication key-pair and save it to the associated user: mkdir ~/.ssh && chmod 700 ~/.ssh On your users machine run the following command to generate the key pair ssh-keygen -b 4096 the copy the public key to the server Windows: scp $env:USERPROFILE/.ssh/id_rsa.pub <username>@<server-address>:<path-to-authorized-keys> Linux: ssh-copy-id <username>@<server-address> MacOS: scp ~/.ssh/id_rsa.pub <username>@<server-address>:<path-to-authorized-keys> Harden SSH We will stop the use of password to login to our server. We edit the file /etc/ssh/sshd_config First copy the file to a backup sudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak Then edit the file sudo nano /etc/ssh/sshd_config update the contents to: # $OpenBSD: sshd_config,v 1.103 2018/04/09 20:41:22 tj Exp $ # This is the sshd server system-wide configuration file. See # sshd_config(5) for more information. # This sshd was compiled with PATH=/usr/bin:/bin:/usr/sbin:/sbin # The strategy used for options in the default sshd_config shipped with # OpenSSH is to specify options with their default value where # possible, but leave them commented. Uncommented options override the # default value. Include /etc/ssh/sshd_config.d/*.conf Port 35020 #AddressFamily any #ListenAddress 0.0.0.0 #ListenAddress :: #HostKey /etc/ssh/ssh_host_rsa_key #HostKey /etc/ssh/ssh_host_ecdsa_key #HostKey /etc/ssh/ssh_host_ed25519_key # Ciphers and keying #RekeyLimit default none # Logging #SyslogFacility AUTH #LogLevel INFO # Authentication: #LoginGraceTime 2m PermitRootLogin no #StrictModes yes MaxAuthTries 4 #MaxSessions 10 #PubkeyAuthentication yes # Expect .ssh/authorized_keys2 to be disregarded by default in future. #AuthorizedKeysFile .ssh/authorized_keys .ssh/authorized_keys2 #AuthorizedPrincipalsFile none #AuthorizedKeysCommand none #AuthorizedKeysCommandUser nobody # For this to work you will also need host keys in /etc/ssh/ssh_known_hosts #HostbasedAuthentication no # Change to yes if you don't trust ~/.ssh/known_hosts for # HostbasedAuthentication #IgnoreUserKnownHosts no # Don't read the user's ~/.rhosts and ~/.shosts files #IgnoreRhosts yes # To disable tunneled clear text passwords, change to no here! PasswordAuthentication no PermitEmptyPasswords no # Change to yes to enable challenge-response passwords (beware issues with # some PAM modules and threads) ChallengeResponseAuthentication no # Kerberos options #KerberosAuthentication no #KerberosOrLocalPasswd yes #KerberosTicketCleanup yes #KerberosGetAFSToken no # GSSAPI options #GSSAPIAuthentication no #GSSAPICleanupCredentials yes #GSSAPIStrictAcceptorCheck yes #GSSAPIKeyExchange no # Set this to 'yes' to enable PAM authentication, account processing, # and session processing. If this is enabled, PAM authentication will # be allowed through the ChallengeResponseAuthentication and # PasswordAuthentication. Depending on your PAM configuration, # PAM authentication via ChallengeResponseAuthentication may bypass # the setting of \"PermitRootLogin without-password\". # If you just want the PAM account and session checks to run without # PAM authentication, then enable this but set PasswordAuthentication # and ChallengeResponseAuthentication to 'no'. UsePAM yes #AllowAgentForwarding yes #AllowTcpForwarding yes #GatewayPorts no X11Forwarding yes #X11DisplayOffset 10 #X11UseLocalhost yes #PermitTTY yes PrintMotd no #PrintLastLog yes #TCPKeepAlive yes #PermitUserEnvironment no #Compression delayed ClientAliveInterval 300 #ClientAliveCountMax 3 #UseDNS no #PidFile /var/run/sshd.pid #MaxStartups 10:30:100 #PermitTunnel no #ChrootDirectory none #VersionAddendum none # no default banner path #Banner none # Allow client to pass locale environment variables AcceptEnv LANG LC_* # override default of no subsystems Subsystem sftp /usr/lib/openssh/sftp-server # Example of overriding settings on a per-user basis #Match User anoncvs # X11Forwarding no # AllowTcpForwarding no # PermitTTY no # ForceCommand cvs server #Permitted users AllowUsers user1 user2 user3 #SSH protocal to use Protocol 2 Restart the ssh service: sudo service restart ssh Firewall Check which ports are being used by the server: sudo ss -tupln Then, check if your firewall is active: sudo ufw status Allow the port that is enabled: sudo ufw allow <port-number>","title":"Secure Linux Server"},{"location":"server-management/security/secure-linux-server/#secure-linux-server-from-hackers","text":"Every server is vulnerable from hackers especially if you have not taken any action to prevent this from happening. So what can you do to protect yourself from a hacker?","title":"Secure Linux Server From Hackers"},{"location":"server-management/security/secure-linux-server/#enable-automatic-updates","text":"Most servers are hacked because they did not have any security updates. To update manually you first check your repositories for updates by running the following command as the root user. apt update To upgrade you run the command apt upgrade To install updates automatically you run: apt install unattended-upgrades to setup automatic upgrades you run: dpkg-reconfigure --priority=low unattended-upgrades","title":"Enable Automatic Updates"},{"location":"server-management/security/secure-linux-server/#limited-user-account","text":"The root user has the ability to make damaging changes to your linux system. So lets limit the user account. As the root user run the following command: adduser <username> Then add user to the sudo group usermod -aG sudo <username>","title":"Limited User Account"},{"location":"server-management/security/secure-linux-server/#ssh-keys","text":"We will create an authentication key-pair and save it to the associated user: mkdir ~/.ssh && chmod 700 ~/.ssh On your users machine run the following command to generate the key pair ssh-keygen -b 4096 the copy the public key to the server Windows: scp $env:USERPROFILE/.ssh/id_rsa.pub <username>@<server-address>:<path-to-authorized-keys> Linux: ssh-copy-id <username>@<server-address> MacOS: scp ~/.ssh/id_rsa.pub <username>@<server-address>:<path-to-authorized-keys>","title":"SSH Keys"},{"location":"server-management/security/secure-linux-server/#harden-ssh","text":"We will stop the use of password to login to our server. We edit the file /etc/ssh/sshd_config First copy the file to a backup sudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak Then edit the file sudo nano /etc/ssh/sshd_config update the contents to: # $OpenBSD: sshd_config,v 1.103 2018/04/09 20:41:22 tj Exp $ # This is the sshd server system-wide configuration file. See # sshd_config(5) for more information. # This sshd was compiled with PATH=/usr/bin:/bin:/usr/sbin:/sbin # The strategy used for options in the default sshd_config shipped with # OpenSSH is to specify options with their default value where # possible, but leave them commented. Uncommented options override the # default value. Include /etc/ssh/sshd_config.d/*.conf Port 35020 #AddressFamily any #ListenAddress 0.0.0.0 #ListenAddress :: #HostKey /etc/ssh/ssh_host_rsa_key #HostKey /etc/ssh/ssh_host_ecdsa_key #HostKey /etc/ssh/ssh_host_ed25519_key # Ciphers and keying #RekeyLimit default none # Logging #SyslogFacility AUTH #LogLevel INFO # Authentication: #LoginGraceTime 2m PermitRootLogin no #StrictModes yes MaxAuthTries 4 #MaxSessions 10 #PubkeyAuthentication yes # Expect .ssh/authorized_keys2 to be disregarded by default in future. #AuthorizedKeysFile .ssh/authorized_keys .ssh/authorized_keys2 #AuthorizedPrincipalsFile none #AuthorizedKeysCommand none #AuthorizedKeysCommandUser nobody # For this to work you will also need host keys in /etc/ssh/ssh_known_hosts #HostbasedAuthentication no # Change to yes if you don't trust ~/.ssh/known_hosts for # HostbasedAuthentication #IgnoreUserKnownHosts no # Don't read the user's ~/.rhosts and ~/.shosts files #IgnoreRhosts yes # To disable tunneled clear text passwords, change to no here! PasswordAuthentication no PermitEmptyPasswords no # Change to yes to enable challenge-response passwords (beware issues with # some PAM modules and threads) ChallengeResponseAuthentication no # Kerberos options #KerberosAuthentication no #KerberosOrLocalPasswd yes #KerberosTicketCleanup yes #KerberosGetAFSToken no # GSSAPI options #GSSAPIAuthentication no #GSSAPICleanupCredentials yes #GSSAPIStrictAcceptorCheck yes #GSSAPIKeyExchange no # Set this to 'yes' to enable PAM authentication, account processing, # and session processing. If this is enabled, PAM authentication will # be allowed through the ChallengeResponseAuthentication and # PasswordAuthentication. Depending on your PAM configuration, # PAM authentication via ChallengeResponseAuthentication may bypass # the setting of \"PermitRootLogin without-password\". # If you just want the PAM account and session checks to run without # PAM authentication, then enable this but set PasswordAuthentication # and ChallengeResponseAuthentication to 'no'. UsePAM yes #AllowAgentForwarding yes #AllowTcpForwarding yes #GatewayPorts no X11Forwarding yes #X11DisplayOffset 10 #X11UseLocalhost yes #PermitTTY yes PrintMotd no #PrintLastLog yes #TCPKeepAlive yes #PermitUserEnvironment no #Compression delayed ClientAliveInterval 300 #ClientAliveCountMax 3 #UseDNS no #PidFile /var/run/sshd.pid #MaxStartups 10:30:100 #PermitTunnel no #ChrootDirectory none #VersionAddendum none # no default banner path #Banner none # Allow client to pass locale environment variables AcceptEnv LANG LC_* # override default of no subsystems Subsystem sftp /usr/lib/openssh/sftp-server # Example of overriding settings on a per-user basis #Match User anoncvs # X11Forwarding no # AllowTcpForwarding no # PermitTTY no # ForceCommand cvs server #Permitted users AllowUsers user1 user2 user3 #SSH protocal to use Protocol 2 Restart the ssh service: sudo service restart ssh","title":"Harden SSH"},{"location":"server-management/security/secure-linux-server/#firewall","text":"Check which ports are being used by the server: sudo ss -tupln Then, check if your firewall is active: sudo ufw status Allow the port that is enabled: sudo ufw allow <port-number>","title":"Firewall"},{"location":"server-management/security/ssl/install-ca-certificate/","text":"Install Certificate Authority Certificate On Windows machine Step 1 Double click on the provided certificate which pops up the window below. Click on install certificate. Step 2 Ensure the certificate will be stored on the current user account on the bellow wizard section Step 3 Manually define where the certficate will be stored by selecting \"Place all certificates in the followinf store\" Step 4 Select the browse button to define where the certificate will be stored Step 5 Ensure you select the \"Trusted Root Certificate Authorities\" store. Step 6 Once that is done click on the finish button and your certificate authorities' certificate will be installed. On Linux machine Have a copy of the ca.crt file on your second Linux system. On Debian and Ubuntu based systems, run the following commands to import the certificate: cp /tmp/ca.crt /usr/local/share/ca-certificates/ update-ca-certificates To import the CA Server\u2019s certificate on CentOS, Fedora, or RedHat based system, copy and paste the file contents onto the system just like in the previous example in a file called /tmp/ca.crt. Next, you\u2019ll copy the certificate into /etc/pki/ca-trust/source/anchors/, then run the update-ca-trust command. sudo cp /tmp/ca.crt /etc/pki/ca-trust/source/anchors/ update-ca-trust On MacOS machine Double click the certificate file. Choose \"System\" from the keychain option. Then press \"OK\". When the following window pops-up, click the \"Always Trust\" button. Then you will notice that the certificate is added to the system entry. On Andriod Phone Your Android device will need to have a PIN set otherwise by default, you will not be able to install any certificates to the device. This can be found under Settings->Security->Screen Lock Click on the SSL certificate file On the \"Name the certificate\" screen gives the certificate a name and press the OK button. To ensure that the installed certificate is Trusted, perform the following steps. The exact location of the Trusted Credentials menu will depend on the version of Android the device is running. Open the Settings App on the device. Go to Security > Trusted Credentials, and tap the User tab If the certificate is present in the list of User certificates, then the certificate installed correctly.","title":"Install Certificate"},{"location":"server-management/security/ssl/install-ca-certificate/#install-certificate-authority-certificate","text":"","title":"Install Certificate Authority Certificate"},{"location":"server-management/security/ssl/install-ca-certificate/#on-windows-machine","text":"","title":"On Windows machine"},{"location":"server-management/security/ssl/install-ca-certificate/#step-1","text":"Double click on the provided certificate which pops up the window below. Click on install certificate.","title":"Step 1"},{"location":"server-management/security/ssl/install-ca-certificate/#step-2","text":"Ensure the certificate will be stored on the current user account on the bellow wizard section","title":"Step 2"},{"location":"server-management/security/ssl/install-ca-certificate/#step-3","text":"Manually define where the certficate will be stored by selecting \"Place all certificates in the followinf store\"","title":"Step 3"},{"location":"server-management/security/ssl/install-ca-certificate/#step-4","text":"Select the browse button to define where the certificate will be stored","title":"Step 4"},{"location":"server-management/security/ssl/install-ca-certificate/#step-5","text":"Ensure you select the \"Trusted Root Certificate Authorities\" store.","title":"Step 5"},{"location":"server-management/security/ssl/install-ca-certificate/#step-6","text":"Once that is done click on the finish button and your certificate authorities' certificate will be installed.","title":"Step 6"},{"location":"server-management/security/ssl/install-ca-certificate/#on-linux-machine","text":"Have a copy of the ca.crt file on your second Linux system. On Debian and Ubuntu based systems, run the following commands to import the certificate: cp /tmp/ca.crt /usr/local/share/ca-certificates/ update-ca-certificates To import the CA Server\u2019s certificate on CentOS, Fedora, or RedHat based system, copy and paste the file contents onto the system just like in the previous example in a file called /tmp/ca.crt. Next, you\u2019ll copy the certificate into /etc/pki/ca-trust/source/anchors/, then run the update-ca-trust command. sudo cp /tmp/ca.crt /etc/pki/ca-trust/source/anchors/ update-ca-trust","title":"On Linux machine"},{"location":"server-management/security/ssl/install-ca-certificate/#on-macos-machine","text":"Double click the certificate file. Choose \"System\" from the keychain option. Then press \"OK\". When the following window pops-up, click the \"Always Trust\" button. Then you will notice that the certificate is added to the system entry.","title":"On MacOS machine"},{"location":"server-management/security/ssl/install-ca-certificate/#on-andriod-phone","text":"Your Android device will need to have a PIN set otherwise by default, you will not be able to install any certificates to the device. This can be found under Settings->Security->Screen Lock Click on the SSL certificate file On the \"Name the certificate\" screen gives the certificate a name and press the OK button. To ensure that the installed certificate is Trusted, perform the following steps. The exact location of the Trusted Credentials menu will depend on the version of Android the device is running. Open the Settings App on the device. Go to Security > Trusted Credentials, and tap the User tab If the certificate is present in the list of User certificates, then the certificate installed correctly.","title":"On Andriod Phone"},{"location":"server-management/security/ssl/ssl-setup/","text":"SSL Communication Between Client and Server SSL stands for Secure Sockets Layer. It is the standard technology for keeping an internet connection secure and safeguarding any sensitive data that is being sent between two systems, preventing criminals from reading and modifying any information transferred, including potential personal details. It does this by making sure that any data transferred between users and sites, or between two systems remain impossible to read. It uses encryption algorithms to scramble data in transit, preventing hackers from reading it as it is sent over the connection. HTTPS (Hyper Text Transfer Protocol Secure) appears in the URL when a website is secured by an SSL certificate. The details of the certificate, including the issuing authority and the corporate name of the website owner, can be viewed by clicking on the lock symbol on the browser bar. An SSL certificate is installed on the server side but there are visual cues on the browser which can tell users that they are protected by SSL. Firstly, if SSL is present on the site, users will see https:// at the start of the web address rather than the http:// (the extra \"s\" stand for \"secure\"). Depending on what level of validation a certificate is given to the business, a secure connection may be indicated by the presence of a padlock icon or a green address bar signal. Transport Layer Security (TLS) is the successor protocol to SSL. TLS is an improved version of SSL. It works in much the same way as the SSL, using encryption to protect the transfer of data and information. The two terms are often used interchangeably in the industry although SSL is still widely used. Configure a Certificate Authority (CA) On Debian 10 A Certificate Authority (CA) is an entity responsible for issuing digital certificates to verify identities on the internet. Although public CAs are a popular choice for verifying the identity of websites and other services that are provided to the general public, private CAs are typically used for closed groups and private services. Building a private Certificate Authority will enable you to configure, test, and run programs that require encrypted connections between a client and a server. With a private CA, you can issue certificates for users, servers, or individual programs and services within your infrastructure. Some examples of programs on Linux that use their own private CA are OpenVPN and Puppet . You can also configure your web server to use certificates issued by a private CA in order to make development and staging environments match production servers that use TLS to encrypt connections. In this guide, we\u2019ll learn how to set up a private Certificate Authority on a Debian 10 server, and how to generate and sign a testing certificate using your new CA. You will also learn how to import the CA server\u2019s public certificate into your operating system\u2019s certificate store so that you can verify the chain of trust between the CA and remote servers or users. Finally you will learn how to revoke certificates and distribute a Certificate Revocation List to make sure only authorized users and systems can use services that rely on your CA. Prerequisites To complete this tutorial, you will need access to a Debian 10 server to host your CA server. You will need to configure a non-root user with sudo privileges before you start this guide. You can follow our Debian 10 initial server setup guide to set up a user with appropriate permissions. The linked tutorial will also set up a firewall, which is assumed to be in place throughout this guide. This server will be referred to as the CA Server in this tutorial. Ensure that the CA Server is a standalone system. It will only be used to import, sign, and revoke certificate requests. It should not run any other services, and ideally it will be offline or completely shut down when you are not actively working with your CA. Note: The last section of this tutorial is optional if you would like to learn about signing and revoking certificates. If you choose to complete those practice steps, you will need a second Debian 10 server or you can also use your own local Linux computer running Debian or Ubuntu, or distributions derived from either of those. Step 1 - Installing Easy-RSA The first task in this tutorial is to install the easy-rsa set of scripts on your CA Server. easy-rsa is a Certificate Authority management tool that you will use to generate a private key, and public root certificate, which you will then use to sign requests from clients and servers that will rely on your CA. Login to your CA Server as the non-root sudo user that you created during the initial setup steps and run the following: sudo apt update sudo apt install easy-rsa You will be prompted to download the package and install it. Press y to confirm you want to install the package. At this point you have everything you need set up and ready to use Easy-RSA. In the next step you will create a Public Key Infrastructure, and then start building your Certificate Authority. Step 2 \u2014 Preparing a Public Key Infrastructure Directory Now that you have installed easy-rsa, it is time to create a skeleton Public Key Infrastructure (PKI) on the CA Server. Ensure that you are still logged in as your non-root user and create an easy-rsa directory. Make sure that you do not use sudo to run any of the following commands, since your normal user should manage and interact with the CA without elevated privileges. mkdir ~/easy-rsa This will create a new directory called easy-rsa in your home folder. We\u2019ll use this directory to create symbolic links pointing to the easy-rsa package files that we\u2019ve installed in the previous step. These files are located in the /usr/share/easy-rsa folder on the CA Server. Create the symlinks with the ln command: ln -s /usr/share/easy-rsa/* ~/easy-rsa/ Note: While other guides might instruct you to copy the easy-rsa package files into your PKI directory, this tutorial adopts a symlink approach. As a result, any updates to the easy-rsa package will be automatically reflected in your PKI\u2019s scripts. To restrict access to your new PKI directory, ensure that only the owner can access it using the chmod command: chmod 700 /home/sammy/easy-rsa Finally, initialize the PKI inside the easy-rsa directory: cd ~/easy-rsa ./easyrsa init-pki Output: init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /home/sammy/easy-rsa/pki After completing this section you have a directory that contains all the files that are needed to create a Certificate Authority. In the next section you will create the private key and public certificate for your CA. Step 3 \u2014 Creating a Certificate Authority Before you can create your CA\u2019s private key and certificate, you need to create and populate a file called vars with some default values. First you will cd into the easy-rsa directory, then you will create and edit the vars file with nano or your preferred text editor: cd ~/easy-rsa nano vars Once the file is opened, paste in the following lines and edit each highlighted value to reflect your own organization info. The important part here is to ensure that you do not leave any of the values blank: set_var EASYRSA_REQ_COUNTRY \"Kenya\" set_var EASYRSA_REQ_PROVINCE \"Nairobi\" set_var EASYRSA_REQ_CITY \"Nairobi City\" set_var EASYRSA_REQ_ORG \"Organization name\" set_var EASYRSA_REQ_EMAIL \"admin@example.com\" set_var EASYRSA_REQ_OU \"Community\" set_var EASYRSA_ALGO \"ec\" set_var EASYRSA_DIGEST \"sha512\" When you are finished, save and close the file. If you are using nano, you can do so by pressing CTRL+X, then Y and ENTER to confirm. You are now ready to build your CA. To create the root public and private key pair for your Certificate Authority, run the ./easy-rsa command again, this time with the build-ca option: ./easyrsa build-ca In the output, you\u2019ll see some lines about the OpenSSL version and you will be prompted to enter a passphrase for your key pair. Be sure to choose a strong passphrase, and note it down somewhere safe. You will need to input the passphrase any time that you need to interact with your CA, for example to sign or revoke a certificate. You will also be asked to confirm the Common Name (CN) for your CA. The CN is the name used to refer to this machine in the context of the Certificate Authority. You can enter any string of characters for the CA\u2019s Common Name but for simplicity\u2019s sake, press ENTER to accept the default name. Output . . . Enter New CA Key Passphrase: Re-Enter New CA Key Passphrase: . . . Common Name (eg: your user, host, or server name) [Easy-RSA CA]: CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /home/sammy/easy-rsa/pki/ca.crt Note: If you don\u2019t want to be prompted for a password every time you interact with your CA, you can run the build-ca command with the nopass option, like this: ```bash ./easyrsa build-ca nopass ``` You now have two important files \u2014 ~/easy-rsa/pki/ca.crt and ~/easy-rsa/pki/private/ca.key \u2014 which make up the public and private components of a Certificate Authority. ca.crt is the CA\u2019s public certificate file. Users, servers, and clients will use this certificate to verify that they are part of the same web of trust. Every user and server that uses your CA will need to have a copy of this file. All parties will rely on the public certificate to ensure that someone is not impersonating a system and performing a Man-in-the-middle attack. ca.key is the private key that the CA uses to sign certificates for servers and clients. If an attacker gains access to your CA and, in turn, your ca.key file, you will need to destroy your CA. This is why your ca.key file should only be on your CA machine and that, ideally, your CA machine should be kept offline when not signing certificate requests as an extra security measure. With that, your CA is in place and it is ready to be used to sign certificate requests, and to revoke certificates. Step 4 \u2014 Distributing your Certificate Authority\u2019s Public Certificate Now your CA is configured and ready to act as a root of trust for any systems that you want to configure to use it. You can add the CA\u2019s certificate to your OpenVPN servers, web servers, mail servers, and so on. Any user or server that needs to verify the identity of another user or server in your network should have a copy of the ca.crt file imported into their operating system\u2019s certificate store. To import the CA\u2019s public certificate into a second Linux system like another server or a local computer, first obtain a copy of the ca.crt file from your CA server. You can use the cat command to output it in a terminal, and then copy and paste it into a file on the second computer that is importing the certificate. You can also use tools like scp, rsync to transfer the file between systems. However we\u2019ll use copy and paste with nano in this step since it will work on all systems. As your non-root user on the CA Server, run the following command: cat ~/easy-rsa/pki/ca.crt There will be output in your terminal that is similar to the following: Output -----BEGIN CERTIFICATE----- MIIDSzCCAjOgAwIBAgIUcR9Crsv3FBEujrPZnZnU4nSb5TMwDQYJKoZIhvcNAQEL BQAwFjEUMBIGA1UEAwwLRWFzeS1SU0EgQ0EwHhcNMjAwMzE4MDMxNjI2WhcNMzAw . . . . . . -----END CERTIFICATE----- Copy everything, including the -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- lines and the dashes. On your second Linux system use nano or your preferred text editor to open a file called /tmp/ca.crt: nano /tmp/ca.crt Paste the contents that you just copied from the CA Server into the editor. When you are finished, save and close the file. If you are using nano, you can do so by pressing CTRL+X, then Y and ENTER to confirm. Now that you have a copy of the ca.crt file on your second Linux system, it is time to import the certificate into its operating system certificate store. On Debian and Ubuntu based systems, run the following commands to import the certificate: cp /tmp/ca.crt /usr/local/share/ca-certificates/ update-ca-certificates To import the CA Server\u2019s certificate on CentOS, Fedora, or RedHat based system, copy and paste the file contents onto the system just like in the previous example in a file called /tmp/ca.crt. Next, you\u2019ll copy the certificate into /etc/pki/ca-trust/source/anchors/, then run the update-ca-trust command. sudo cp /tmp/ca.crt /etc/pki/ca-trust/source/anchors/ update-ca-trust Now your second Linux system will trust any certificate that has been signed by the CA server. Note: If you are using your CA with web servers and use Firefox as a browser you will need to import the public ca.crt certificate into Firefox directly. Firefox does not use the local operating system\u2019s certificate store. For details on how to add your CA\u2019s certificate to Firefox please see this support article from Mozilla on Setting Up Certificate Authorities (CAs) in Firefox. If you are using your CA to integrate with a Windows environment or desktop computers, please see the documentation on how to use certutil.exe to install a CA certificate. If you are using this tutorial as a prerequisite for another tutorial, or are familiar with how to sign and revoke certificates you can stop here. If you would like to learn more about how to sign and revoke certificates, then the following optional section will explain each process in detail. (Optional) \u2014 Creating Certificate Signing Requests and Revoking Certificates The following sections of the tutorial are optional. If you have completed all the previous steps then you have a fully configured and working Certificate Authority that you can use as a prerequisite for other tutorials. You can import your CA\u2019s ca.crt file and verify certificates in your network that have been signed by your CA. If you would like to practice and learn more about how to sign certificate requests, and how to revoke certificates, then these optional sections will explain how both processes work. (Optional) \u2014 Creating and Signing a Practice Certificate Request Now that you have a CA ready to use, you can practice generating a private key and certificate request to get familiar with the signing and distribution process. A Certificate Signing Request (CSR) consists of three parts: a public key, identifying information about the requesting system, and a signature of the request itself, which is created using the requesting party\u2019s private key. The private key will be kept secret, and will be used to encrypt information that anyone with the signed public certificate can then decrypt. The following steps will be run on your second Linux system Debian, Ubuntu, or distribution that is derived from either of those. It can be another remote server, or a local Linux machine like a laptop or a desktop computer. Since easy-rsa is not available by default on all systems, we\u2019ll use the openssl tool to create a practice private key and certificate. openssl is usually installed by default on most Linux distributions, but just to be certain, run the following on your system: sudo apt update sudo apt install openssl When you are prompted to install openssl enter y to continue with the installation steps. Now you are ready to create a practice CSR with openssl. The first step that you need to complete to create a CSR is generating a private key. To create a private key using openssl, create a practice-csr directory and then generate a key inside it. We will make this request for a fictional server called sammy-server, as opposed to creating a certificate that is used to identify a user or another CA. mkdir ~/practice-csr cd ~/practice-csr openssl genrsa -out sammy-server.key Output Generating RSA private key, 2048 bit long modulus (2 primes) . . . . . . e is 65537 (0x010001) Now that you have a private key you can create a corresponding CSR, again using the openssl utility. You will be prompted to fill out a number of fields like Country, State, and City. You can enter a . if you\u2019d like to leave a field blank, but be aware that if this were a real CSR, it is best to use the correct values for your location and organization: openssl req -new -key sammy-server.key -out sammy-server.req Output . . . ----- Country Name (2 letter code) [XX]:US State or Province Name (full name) []:New York Locality Name (eg, city) [Default City]:New York City Organization Name (eg, company) [Default Company Ltd]:DigitalOcean Organizational Unit Name (eg, section) []:Community Common Name (eg, your name or your server's hostname) []:sammy-server Email Address []: Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []: An optional company name []: If you would like to automatically add those values as part of the openssl invocation instead of via the interactive prompt, you can pass the -subj argument to OpenSSL. Be sure to edit the highlighted values to match your practice location, organization, and server name: openssl req -new -key sammy-server.key -out server.req -subj \\ /C=US/ST=New\\ York/L=New\\ York\\ City/O=DigitalOcean/OU=Community/CN=sammy-server To verify the contents of a CSR, you can read in a request file with openssl and examine the fields inside: openssl req -in sammy-server.req -noout -subject Output subject=C = US, ST = New York, L = New York City, O = DigitalOcean, OU = Community, CN = sammy-server Once you\u2019re happy with the subject of your practice certificate request, copy the sammy-server.req file to your CA server using scp: scp sammy-server.req sammy@your_ca_server_ip:/tmp/sammy-server.req In this step you generated a Certificate Signing Request for a fictional server called sammy-server. In a real-world scenario, the request could be from something like a staging or development web server that needs a TLS certificate for testing; or it could come from an OpenVPN server that is requesting a certificate so that users can connect to a VPN. In the next step, we\u2019ll proceed to signing the certificate signing request using the CA Server\u2019s private key. (Optional) \u2014 Signing a CSR In the previous step, you created a practice certificate request and key for a fictional server. You copied it to the /tmp directory on your CA server, emulating the process that you would use if you had real clients or servers sending you CSR requests that need to be signed. Continuing with the fictional scenario, now the CA Server needs to import the practice certificate and sign it. Once a certificate request is validated by the CA and relayed back to a server, clients that trust the Certificate Authority will also be able to trust the newly issued certificate. Since we will be operating inside the CA\u2019s PKI where the easy-rsa utility is available, the signing steps will use the easy-rsa utility to make things easier, as opposed to using the openssl directly like we did in the previous example. The first step to sign the fictional CSR is to import the certificate request using the easy-rsa script: cd ~/easy-rsa ./easyrsa import-req /tmp/sammy-server.req sammy-server Output . . . The request has been successfully imported with a short name of: sammy-server You may now use this name to perform signing operations on this request. Now you can sign the request by running the easyrsa script with the sign-req option, followed by the request type and the Common Name that is included in the CSR. The request type can either be one of client, server, or ca. Since we\u2019re practicing with a certificate for a fictional server, be sure to use the server request type: ./easyrsa sign-req server sammy-server In the output, you\u2019ll be asked to verify that the request comes from a trusted source. Type yes then press ENTER to confirm this: Output You are about to sign the following certificate. Please check over the details shown below for accuracy. Note that this request has not been cryptographically verified. Please be sure it came from a trusted source or that you have verified the request checksum with the sender. Request subject, to be signed as a server certificate for 3650 days: subject= commonName = sammy-server Type the word 'yes' to continue, or any other input to abort. Confirm request details: yes . . . Certificate created at: /home/sammy/easy-rsa/pki/issued/sammy-server.crt If you encrypted your CA key, you\u2019ll be prompted for your password at this point. With those steps complete, you have signed the sammy-server.req CSR using the CA Server\u2019s private key in /home/sammy/easy-rsa/pki/private/ca.key. The resulting sammy-server.crt file contains the practice server\u2019s public encryption key, as well as a new signature from the CA Server. The point of the signature is to tell anyone who trusts the CA that they can also trust the sammy-server certificate. If this request was for a real server like a web server or VPN server, the last step on the CA Server would be to distribute the new sammy-server.crt and ca.crt files from the CA Server to the remote server that made the CSR request: scp pki/issued/sammy-server.crt sammy@your_server_ip:/tmp scp pki/ca.crt sammy@your_server_ip:/tmp At this point, you would be able to use the issued certificate with something like a web server, a VPN, configuration management tool, database system, or for client authentication purposes. (Optional) \u2014 Revoking a Certificate Occasionally, you may need to revoke a certificate to prevent a user or server from using it. Perhaps someone\u2019s laptop was stolen, a web server was compromised, or an employee or contractor has left your organization. To revoke a certificate, the general process follows these steps: Revoke the certificate with the ./easyrsa revoke client_name command. Generate a new CRL with the ./easyrsa gen-crl command. Transfer the updated crl.pem file to the server or servers that rely on your CA, and on those systems copy it to the required directory or directories for programs that refer to it. Restart any services that use your CA and the CRL file. You can use this process to revoke any certificates that you\u2019ve previously issued at any time. We\u2019ll go over each step in detail in the following sections, starting with the revoke command. Revoking a Certificate To revoke a certificate, navigate to the easy-rsa directory on your CA server: cd ~/easy-rsa Next, run the easyrsa script with the revoke option, followed by the client name you wish to revoke. Following the practice example above, the Common Name of the certificate is sammy-server: ./easyrsa revoke sammy-server This will ask you to confirm the revocation by entering yes: Output Please confirm you wish to revoke the certificate with the following subject: subject= commonName = sammy-server Type the word 'yes' to continue, or any other input to abort. Continue with revocation: yes . . . Revoking Certificate 8348B3F146A765581946040D5C4D590A . . . Note the highlighted value on the Revoking Certificate line. This value is the unique serial number of the certificate that is being revoked. If you want to examine the revocation list in the last step of this section to verify that the certificate is in it, you\u2019ll need this value. After confirming the action, the CA will revoke the certificate. However, remote systems that rely on the CA have no way to check whether any certificates have been revoked. Users and servers will still be able to use the certificate until the CA\u2019s Certificate Revocation List (CRL) is distributed to all systems that rely on the CA. In the next step you\u2019ll generate a CRL or update an existing crl.pem file. Generating a Certificate Revocation List Now that you have revoked a certificate, it is important to update the list of revoked certificates on your CA server. Once you have an updated revocation list you will be able to tell which users and systems have valid certificates in your CA. To generate a CRL, run the easy-rsa command with the gen-crl option while still inside the ~/easy-rsa directory: ./easyrsa gen-crl If you have used a passphrase when creating your ca.key file, you will be prompted to enter it. The gen-crl command will generate a file called crl.pem, containing the updated list of revoked certificates for that CA. Next you\u2019ll need to transfer the updated crl.pem file to all servers and clients that rely on this CA each time you run the gen-crl command. Otherwise, clients and systems will still be able to access services and systems that use your CA, since those services need to know about the revoked status of the certificate. Transferring a Certificate Revocation List Now that you have generated a CRL on your CA server, you need to transfer it to remote systems that rely on your CA. To transfer this file to your servers, you can use the scp command. Note: This tutorial explains how to generate and distribute a CRL manually. While there are more robust and automated methods to distribute and check revocation lists like OCSP-Stapling, configuring those methods is beyond the scope of this article. Ensure you are logged into your CA server as your non-root user and run the following, substituting in your own server IP or DNS name in place of your_server_ip: scp ~/easy-rsa/pki/crl.pem sammy@your_server_ip:/tmp Now that the file is on the remote system, the last step is to update any services with the new copy of the revocation list. Updating Services that Support a CRL Listing the steps that you need to use to update services that use the crl.pem file is beyond the scope of this tutorial. In general you will need to copy the crl.pem file into the location that the service expects and then restart it using systemctl. Once you have updated your services with the new crl.pem file, your services will be able to reject connections from clients or servers that are using a revoked certificate. Examining and Verifying the Contents of a CRL If you would like to examine a CRL file, for example to confirm a list of revoked certificates, use the following openssl command from within your easy-rsa directory on your CA server: cd ~/easy-rsa openssl crl -in pki/crl.pem -noout -text You can also run this command on any server or system that has the openssl tool installed with a copy of the crl.pem file. For example, if you transferred the crl.pem file to your second system and want to verify that the sammy-server certificate is revoked, you can use an openssl command like the following, substituting the serial number that you noted earlier when you revoked the certificate in place of the highlighted one here: openssl crl -in /tmp/crl.pem -noout -text |grep -A 1 8348B3F146A765581946040D5C4D590A Output Serial Number: 8348B3F146A765581946040D5C4D590A Revocation Date: Apr 1 20:48:02 2020 GMT Notice how the grep command is used to check for the unique serial number that you noted in the revocation step. Now you can verify the contents of your Certificate Revocation List on any system that relies on it to restrict access to users and services. Conclusion In this tutorial you created a private Certificate Authority using the Easy-RSA package on a standalone Debian 10 server. You learned how the trust model works between parties that rely on the CA. You also created and signed a Certificate Signing Request (CSR) for a practice server and then learned how to revoke a certificate. Finally, you learned how to generate and distribute a Certificate Revocation List (CRL) for any system that relies on your CA to ensure that users or servers that should not access services are prevented from doing so. Now you can issue certificates for users and use them with services like OpenVPN. You can also use your CA to configure development and staging web servers with certificates to secure your non-production environments. Using a CA with TLS certificates during development can help ensure that your code and environments match your production environment as closely as possible. If you would like to learn more about how to use OpenSSL, our OpenSSL Essentials: Working with SSL Certificates, Private Keys and CSRs tutorial has lots of additional information to help you become more familiar with OpenSSL fundamentals.","title":"Setup"},{"location":"server-management/security/ssl/ssl-setup/#ssl-communication-between-client-and-server","text":"SSL stands for Secure Sockets Layer. It is the standard technology for keeping an internet connection secure and safeguarding any sensitive data that is being sent between two systems, preventing criminals from reading and modifying any information transferred, including potential personal details. It does this by making sure that any data transferred between users and sites, or between two systems remain impossible to read. It uses encryption algorithms to scramble data in transit, preventing hackers from reading it as it is sent over the connection. HTTPS (Hyper Text Transfer Protocol Secure) appears in the URL when a website is secured by an SSL certificate. The details of the certificate, including the issuing authority and the corporate name of the website owner, can be viewed by clicking on the lock symbol on the browser bar. An SSL certificate is installed on the server side but there are visual cues on the browser which can tell users that they are protected by SSL. Firstly, if SSL is present on the site, users will see https:// at the start of the web address rather than the http:// (the extra \"s\" stand for \"secure\"). Depending on what level of validation a certificate is given to the business, a secure connection may be indicated by the presence of a padlock icon or a green address bar signal. Transport Layer Security (TLS) is the successor protocol to SSL. TLS is an improved version of SSL. It works in much the same way as the SSL, using encryption to protect the transfer of data and information. The two terms are often used interchangeably in the industry although SSL is still widely used.","title":"SSL Communication Between Client and Server"},{"location":"server-management/security/ssl/ssl-setup/#configure-a-certificate-authority-ca-on-debian-10","text":"A Certificate Authority (CA) is an entity responsible for issuing digital certificates to verify identities on the internet. Although public CAs are a popular choice for verifying the identity of websites and other services that are provided to the general public, private CAs are typically used for closed groups and private services. Building a private Certificate Authority will enable you to configure, test, and run programs that require encrypted connections between a client and a server. With a private CA, you can issue certificates for users, servers, or individual programs and services within your infrastructure. Some examples of programs on Linux that use their own private CA are OpenVPN and Puppet . You can also configure your web server to use certificates issued by a private CA in order to make development and staging environments match production servers that use TLS to encrypt connections. In this guide, we\u2019ll learn how to set up a private Certificate Authority on a Debian 10 server, and how to generate and sign a testing certificate using your new CA. You will also learn how to import the CA server\u2019s public certificate into your operating system\u2019s certificate store so that you can verify the chain of trust between the CA and remote servers or users. Finally you will learn how to revoke certificates and distribute a Certificate Revocation List to make sure only authorized users and systems can use services that rely on your CA.","title":"Configure a Certificate Authority (CA) On Debian 10"},{"location":"server-management/security/ssl/ssl-setup/#prerequisites","text":"To complete this tutorial, you will need access to a Debian 10 server to host your CA server. You will need to configure a non-root user with sudo privileges before you start this guide. You can follow our Debian 10 initial server setup guide to set up a user with appropriate permissions. The linked tutorial will also set up a firewall, which is assumed to be in place throughout this guide. This server will be referred to as the CA Server in this tutorial. Ensure that the CA Server is a standalone system. It will only be used to import, sign, and revoke certificate requests. It should not run any other services, and ideally it will be offline or completely shut down when you are not actively working with your CA. Note: The last section of this tutorial is optional if you would like to learn about signing and revoking certificates. If you choose to complete those practice steps, you will need a second Debian 10 server or you can also use your own local Linux computer running Debian or Ubuntu, or distributions derived from either of those.","title":"Prerequisites"},{"location":"server-management/security/ssl/ssl-setup/#step-1-installing-easy-rsa","text":"The first task in this tutorial is to install the easy-rsa set of scripts on your CA Server. easy-rsa is a Certificate Authority management tool that you will use to generate a private key, and public root certificate, which you will then use to sign requests from clients and servers that will rely on your CA. Login to your CA Server as the non-root sudo user that you created during the initial setup steps and run the following: sudo apt update sudo apt install easy-rsa You will be prompted to download the package and install it. Press y to confirm you want to install the package. At this point you have everything you need set up and ready to use Easy-RSA. In the next step you will create a Public Key Infrastructure, and then start building your Certificate Authority.","title":"Step 1 - Installing Easy-RSA"},{"location":"server-management/security/ssl/ssl-setup/#step-2-preparing-a-public-key-infrastructure-directory","text":"Now that you have installed easy-rsa, it is time to create a skeleton Public Key Infrastructure (PKI) on the CA Server. Ensure that you are still logged in as your non-root user and create an easy-rsa directory. Make sure that you do not use sudo to run any of the following commands, since your normal user should manage and interact with the CA without elevated privileges. mkdir ~/easy-rsa This will create a new directory called easy-rsa in your home folder. We\u2019ll use this directory to create symbolic links pointing to the easy-rsa package files that we\u2019ve installed in the previous step. These files are located in the /usr/share/easy-rsa folder on the CA Server. Create the symlinks with the ln command: ln -s /usr/share/easy-rsa/* ~/easy-rsa/ Note: While other guides might instruct you to copy the easy-rsa package files into your PKI directory, this tutorial adopts a symlink approach. As a result, any updates to the easy-rsa package will be automatically reflected in your PKI\u2019s scripts. To restrict access to your new PKI directory, ensure that only the owner can access it using the chmod command: chmod 700 /home/sammy/easy-rsa Finally, initialize the PKI inside the easy-rsa directory: cd ~/easy-rsa ./easyrsa init-pki Output: init-pki complete; you may now create a CA or requests. Your newly created PKI dir is: /home/sammy/easy-rsa/pki After completing this section you have a directory that contains all the files that are needed to create a Certificate Authority. In the next section you will create the private key and public certificate for your CA.","title":"Step 2 \u2014 Preparing a Public Key Infrastructure Directory"},{"location":"server-management/security/ssl/ssl-setup/#step-3-creating-a-certificate-authority","text":"Before you can create your CA\u2019s private key and certificate, you need to create and populate a file called vars with some default values. First you will cd into the easy-rsa directory, then you will create and edit the vars file with nano or your preferred text editor: cd ~/easy-rsa nano vars Once the file is opened, paste in the following lines and edit each highlighted value to reflect your own organization info. The important part here is to ensure that you do not leave any of the values blank: set_var EASYRSA_REQ_COUNTRY \"Kenya\" set_var EASYRSA_REQ_PROVINCE \"Nairobi\" set_var EASYRSA_REQ_CITY \"Nairobi City\" set_var EASYRSA_REQ_ORG \"Organization name\" set_var EASYRSA_REQ_EMAIL \"admin@example.com\" set_var EASYRSA_REQ_OU \"Community\" set_var EASYRSA_ALGO \"ec\" set_var EASYRSA_DIGEST \"sha512\" When you are finished, save and close the file. If you are using nano, you can do so by pressing CTRL+X, then Y and ENTER to confirm. You are now ready to build your CA. To create the root public and private key pair for your Certificate Authority, run the ./easy-rsa command again, this time with the build-ca option: ./easyrsa build-ca In the output, you\u2019ll see some lines about the OpenSSL version and you will be prompted to enter a passphrase for your key pair. Be sure to choose a strong passphrase, and note it down somewhere safe. You will need to input the passphrase any time that you need to interact with your CA, for example to sign or revoke a certificate. You will also be asked to confirm the Common Name (CN) for your CA. The CN is the name used to refer to this machine in the context of the Certificate Authority. You can enter any string of characters for the CA\u2019s Common Name but for simplicity\u2019s sake, press ENTER to accept the default name. Output . . . Enter New CA Key Passphrase: Re-Enter New CA Key Passphrase: . . . Common Name (eg: your user, host, or server name) [Easy-RSA CA]: CA creation complete and you may now import and sign cert requests. Your new CA certificate file for publishing is at: /home/sammy/easy-rsa/pki/ca.crt Note: If you don\u2019t want to be prompted for a password every time you interact with your CA, you can run the build-ca command with the nopass option, like this: ```bash ./easyrsa build-ca nopass ``` You now have two important files \u2014 ~/easy-rsa/pki/ca.crt and ~/easy-rsa/pki/private/ca.key \u2014 which make up the public and private components of a Certificate Authority. ca.crt is the CA\u2019s public certificate file. Users, servers, and clients will use this certificate to verify that they are part of the same web of trust. Every user and server that uses your CA will need to have a copy of this file. All parties will rely on the public certificate to ensure that someone is not impersonating a system and performing a Man-in-the-middle attack. ca.key is the private key that the CA uses to sign certificates for servers and clients. If an attacker gains access to your CA and, in turn, your ca.key file, you will need to destroy your CA. This is why your ca.key file should only be on your CA machine and that, ideally, your CA machine should be kept offline when not signing certificate requests as an extra security measure. With that, your CA is in place and it is ready to be used to sign certificate requests, and to revoke certificates.","title":"Step 3 \u2014 Creating a Certificate Authority"},{"location":"server-management/security/ssl/ssl-setup/#step-4-distributing-your-certificate-authoritys-public-certificate","text":"Now your CA is configured and ready to act as a root of trust for any systems that you want to configure to use it. You can add the CA\u2019s certificate to your OpenVPN servers, web servers, mail servers, and so on. Any user or server that needs to verify the identity of another user or server in your network should have a copy of the ca.crt file imported into their operating system\u2019s certificate store. To import the CA\u2019s public certificate into a second Linux system like another server or a local computer, first obtain a copy of the ca.crt file from your CA server. You can use the cat command to output it in a terminal, and then copy and paste it into a file on the second computer that is importing the certificate. You can also use tools like scp, rsync to transfer the file between systems. However we\u2019ll use copy and paste with nano in this step since it will work on all systems. As your non-root user on the CA Server, run the following command: cat ~/easy-rsa/pki/ca.crt There will be output in your terminal that is similar to the following: Output -----BEGIN CERTIFICATE----- MIIDSzCCAjOgAwIBAgIUcR9Crsv3FBEujrPZnZnU4nSb5TMwDQYJKoZIhvcNAQEL BQAwFjEUMBIGA1UEAwwLRWFzeS1SU0EgQ0EwHhcNMjAwMzE4MDMxNjI2WhcNMzAw . . . . . . -----END CERTIFICATE----- Copy everything, including the -----BEGIN CERTIFICATE----- and -----END CERTIFICATE----- lines and the dashes. On your second Linux system use nano or your preferred text editor to open a file called /tmp/ca.crt: nano /tmp/ca.crt Paste the contents that you just copied from the CA Server into the editor. When you are finished, save and close the file. If you are using nano, you can do so by pressing CTRL+X, then Y and ENTER to confirm. Now that you have a copy of the ca.crt file on your second Linux system, it is time to import the certificate into its operating system certificate store. On Debian and Ubuntu based systems, run the following commands to import the certificate: cp /tmp/ca.crt /usr/local/share/ca-certificates/ update-ca-certificates To import the CA Server\u2019s certificate on CentOS, Fedora, or RedHat based system, copy and paste the file contents onto the system just like in the previous example in a file called /tmp/ca.crt. Next, you\u2019ll copy the certificate into /etc/pki/ca-trust/source/anchors/, then run the update-ca-trust command. sudo cp /tmp/ca.crt /etc/pki/ca-trust/source/anchors/ update-ca-trust Now your second Linux system will trust any certificate that has been signed by the CA server. Note: If you are using your CA with web servers and use Firefox as a browser you will need to import the public ca.crt certificate into Firefox directly. Firefox does not use the local operating system\u2019s certificate store. For details on how to add your CA\u2019s certificate to Firefox please see this support article from Mozilla on Setting Up Certificate Authorities (CAs) in Firefox. If you are using your CA to integrate with a Windows environment or desktop computers, please see the documentation on how to use certutil.exe to install a CA certificate. If you are using this tutorial as a prerequisite for another tutorial, or are familiar with how to sign and revoke certificates you can stop here. If you would like to learn more about how to sign and revoke certificates, then the following optional section will explain each process in detail.","title":"Step 4 \u2014 Distributing your Certificate Authority\u2019s Public Certificate"},{"location":"server-management/security/ssl/ssl-setup/#optional-creating-certificate-signing-requests-and-revoking-certificates","text":"The following sections of the tutorial are optional. If you have completed all the previous steps then you have a fully configured and working Certificate Authority that you can use as a prerequisite for other tutorials. You can import your CA\u2019s ca.crt file and verify certificates in your network that have been signed by your CA. If you would like to practice and learn more about how to sign certificate requests, and how to revoke certificates, then these optional sections will explain how both processes work.","title":"(Optional) \u2014 Creating Certificate Signing Requests and Revoking Certificates"},{"location":"server-management/security/ssl/ssl-setup/#optional-creating-and-signing-a-practice-certificate-request","text":"Now that you have a CA ready to use, you can practice generating a private key and certificate request to get familiar with the signing and distribution process. A Certificate Signing Request (CSR) consists of three parts: a public key, identifying information about the requesting system, and a signature of the request itself, which is created using the requesting party\u2019s private key. The private key will be kept secret, and will be used to encrypt information that anyone with the signed public certificate can then decrypt. The following steps will be run on your second Linux system Debian, Ubuntu, or distribution that is derived from either of those. It can be another remote server, or a local Linux machine like a laptop or a desktop computer. Since easy-rsa is not available by default on all systems, we\u2019ll use the openssl tool to create a practice private key and certificate. openssl is usually installed by default on most Linux distributions, but just to be certain, run the following on your system: sudo apt update sudo apt install openssl When you are prompted to install openssl enter y to continue with the installation steps. Now you are ready to create a practice CSR with openssl. The first step that you need to complete to create a CSR is generating a private key. To create a private key using openssl, create a practice-csr directory and then generate a key inside it. We will make this request for a fictional server called sammy-server, as opposed to creating a certificate that is used to identify a user or another CA. mkdir ~/practice-csr cd ~/practice-csr openssl genrsa -out sammy-server.key Output Generating RSA private key, 2048 bit long modulus (2 primes) . . . . . . e is 65537 (0x010001) Now that you have a private key you can create a corresponding CSR, again using the openssl utility. You will be prompted to fill out a number of fields like Country, State, and City. You can enter a . if you\u2019d like to leave a field blank, but be aware that if this were a real CSR, it is best to use the correct values for your location and organization: openssl req -new -key sammy-server.key -out sammy-server.req Output . . . ----- Country Name (2 letter code) [XX]:US State or Province Name (full name) []:New York Locality Name (eg, city) [Default City]:New York City Organization Name (eg, company) [Default Company Ltd]:DigitalOcean Organizational Unit Name (eg, section) []:Community Common Name (eg, your name or your server's hostname) []:sammy-server Email Address []: Please enter the following 'extra' attributes to be sent with your certificate request A challenge password []: An optional company name []: If you would like to automatically add those values as part of the openssl invocation instead of via the interactive prompt, you can pass the -subj argument to OpenSSL. Be sure to edit the highlighted values to match your practice location, organization, and server name: openssl req -new -key sammy-server.key -out server.req -subj \\ /C=US/ST=New\\ York/L=New\\ York\\ City/O=DigitalOcean/OU=Community/CN=sammy-server To verify the contents of a CSR, you can read in a request file with openssl and examine the fields inside: openssl req -in sammy-server.req -noout -subject Output subject=C = US, ST = New York, L = New York City, O = DigitalOcean, OU = Community, CN = sammy-server Once you\u2019re happy with the subject of your practice certificate request, copy the sammy-server.req file to your CA server using scp: scp sammy-server.req sammy@your_ca_server_ip:/tmp/sammy-server.req In this step you generated a Certificate Signing Request for a fictional server called sammy-server. In a real-world scenario, the request could be from something like a staging or development web server that needs a TLS certificate for testing; or it could come from an OpenVPN server that is requesting a certificate so that users can connect to a VPN. In the next step, we\u2019ll proceed to signing the certificate signing request using the CA Server\u2019s private key.","title":"(Optional) \u2014 Creating and Signing a Practice Certificate Request"},{"location":"server-management/security/ssl/ssl-setup/#optional-signing-a-csr","text":"In the previous step, you created a practice certificate request and key for a fictional server. You copied it to the /tmp directory on your CA server, emulating the process that you would use if you had real clients or servers sending you CSR requests that need to be signed. Continuing with the fictional scenario, now the CA Server needs to import the practice certificate and sign it. Once a certificate request is validated by the CA and relayed back to a server, clients that trust the Certificate Authority will also be able to trust the newly issued certificate. Since we will be operating inside the CA\u2019s PKI where the easy-rsa utility is available, the signing steps will use the easy-rsa utility to make things easier, as opposed to using the openssl directly like we did in the previous example. The first step to sign the fictional CSR is to import the certificate request using the easy-rsa script: cd ~/easy-rsa ./easyrsa import-req /tmp/sammy-server.req sammy-server Output . . . The request has been successfully imported with a short name of: sammy-server You may now use this name to perform signing operations on this request. Now you can sign the request by running the easyrsa script with the sign-req option, followed by the request type and the Common Name that is included in the CSR. The request type can either be one of client, server, or ca. Since we\u2019re practicing with a certificate for a fictional server, be sure to use the server request type: ./easyrsa sign-req server sammy-server In the output, you\u2019ll be asked to verify that the request comes from a trusted source. Type yes then press ENTER to confirm this: Output You are about to sign the following certificate. Please check over the details shown below for accuracy. Note that this request has not been cryptographically verified. Please be sure it came from a trusted source or that you have verified the request checksum with the sender. Request subject, to be signed as a server certificate for 3650 days: subject= commonName = sammy-server Type the word 'yes' to continue, or any other input to abort. Confirm request details: yes . . . Certificate created at: /home/sammy/easy-rsa/pki/issued/sammy-server.crt If you encrypted your CA key, you\u2019ll be prompted for your password at this point. With those steps complete, you have signed the sammy-server.req CSR using the CA Server\u2019s private key in /home/sammy/easy-rsa/pki/private/ca.key. The resulting sammy-server.crt file contains the practice server\u2019s public encryption key, as well as a new signature from the CA Server. The point of the signature is to tell anyone who trusts the CA that they can also trust the sammy-server certificate. If this request was for a real server like a web server or VPN server, the last step on the CA Server would be to distribute the new sammy-server.crt and ca.crt files from the CA Server to the remote server that made the CSR request: scp pki/issued/sammy-server.crt sammy@your_server_ip:/tmp scp pki/ca.crt sammy@your_server_ip:/tmp At this point, you would be able to use the issued certificate with something like a web server, a VPN, configuration management tool, database system, or for client authentication purposes.","title":"(Optional) \u2014 Signing a CSR"},{"location":"server-management/security/ssl/ssl-setup/#optional-revoking-a-certificate","text":"Occasionally, you may need to revoke a certificate to prevent a user or server from using it. Perhaps someone\u2019s laptop was stolen, a web server was compromised, or an employee or contractor has left your organization. To revoke a certificate, the general process follows these steps: Revoke the certificate with the ./easyrsa revoke client_name command. Generate a new CRL with the ./easyrsa gen-crl command. Transfer the updated crl.pem file to the server or servers that rely on your CA, and on those systems copy it to the required directory or directories for programs that refer to it. Restart any services that use your CA and the CRL file. You can use this process to revoke any certificates that you\u2019ve previously issued at any time. We\u2019ll go over each step in detail in the following sections, starting with the revoke command.","title":"(Optional) \u2014 Revoking a Certificate"},{"location":"server-management/security/ssl/ssl-setup/#revoking-a-certificate","text":"To revoke a certificate, navigate to the easy-rsa directory on your CA server: cd ~/easy-rsa Next, run the easyrsa script with the revoke option, followed by the client name you wish to revoke. Following the practice example above, the Common Name of the certificate is sammy-server: ./easyrsa revoke sammy-server This will ask you to confirm the revocation by entering yes: Output Please confirm you wish to revoke the certificate with the following subject: subject= commonName = sammy-server Type the word 'yes' to continue, or any other input to abort. Continue with revocation: yes . . . Revoking Certificate 8348B3F146A765581946040D5C4D590A . . . Note the highlighted value on the Revoking Certificate line. This value is the unique serial number of the certificate that is being revoked. If you want to examine the revocation list in the last step of this section to verify that the certificate is in it, you\u2019ll need this value. After confirming the action, the CA will revoke the certificate. However, remote systems that rely on the CA have no way to check whether any certificates have been revoked. Users and servers will still be able to use the certificate until the CA\u2019s Certificate Revocation List (CRL) is distributed to all systems that rely on the CA. In the next step you\u2019ll generate a CRL or update an existing crl.pem file. Generating a Certificate Revocation List Now that you have revoked a certificate, it is important to update the list of revoked certificates on your CA server. Once you have an updated revocation list you will be able to tell which users and systems have valid certificates in your CA. To generate a CRL, run the easy-rsa command with the gen-crl option while still inside the ~/easy-rsa directory: ./easyrsa gen-crl If you have used a passphrase when creating your ca.key file, you will be prompted to enter it. The gen-crl command will generate a file called crl.pem, containing the updated list of revoked certificates for that CA. Next you\u2019ll need to transfer the updated crl.pem file to all servers and clients that rely on this CA each time you run the gen-crl command. Otherwise, clients and systems will still be able to access services and systems that use your CA, since those services need to know about the revoked status of the certificate.","title":"Revoking a Certificate"},{"location":"server-management/security/ssl/ssl-setup/#transferring-a-certificate-revocation-list","text":"Now that you have generated a CRL on your CA server, you need to transfer it to remote systems that rely on your CA. To transfer this file to your servers, you can use the scp command. Note: This tutorial explains how to generate and distribute a CRL manually. While there are more robust and automated methods to distribute and check revocation lists like OCSP-Stapling, configuring those methods is beyond the scope of this article. Ensure you are logged into your CA server as your non-root user and run the following, substituting in your own server IP or DNS name in place of your_server_ip: scp ~/easy-rsa/pki/crl.pem sammy@your_server_ip:/tmp Now that the file is on the remote system, the last step is to update any services with the new copy of the revocation list.","title":"Transferring a Certificate Revocation List"},{"location":"server-management/security/ssl/ssl-setup/#updating-services-that-support-a-crl","text":"Listing the steps that you need to use to update services that use the crl.pem file is beyond the scope of this tutorial. In general you will need to copy the crl.pem file into the location that the service expects and then restart it using systemctl. Once you have updated your services with the new crl.pem file, your services will be able to reject connections from clients or servers that are using a revoked certificate.","title":"Updating Services that Support a CRL"},{"location":"server-management/security/ssl/ssl-setup/#examining-and-verifying-the-contents-of-a-crl","text":"If you would like to examine a CRL file, for example to confirm a list of revoked certificates, use the following openssl command from within your easy-rsa directory on your CA server: cd ~/easy-rsa openssl crl -in pki/crl.pem -noout -text You can also run this command on any server or system that has the openssl tool installed with a copy of the crl.pem file. For example, if you transferred the crl.pem file to your second system and want to verify that the sammy-server certificate is revoked, you can use an openssl command like the following, substituting the serial number that you noted earlier when you revoked the certificate in place of the highlighted one here: openssl crl -in /tmp/crl.pem -noout -text |grep -A 1 8348B3F146A765581946040D5C4D590A Output Serial Number: 8348B3F146A765581946040D5C4D590A Revocation Date: Apr 1 20:48:02 2020 GMT Notice how the grep command is used to check for the unique serial number that you noted in the revocation step. Now you can verify the contents of your Certificate Revocation List on any system that relies on it to restrict access to users and services.","title":"Examining and Verifying the Contents of a CRL"},{"location":"server-management/security/ssl/ssl-setup/#conclusion","text":"In this tutorial you created a private Certificate Authority using the Easy-RSA package on a standalone Debian 10 server. You learned how the trust model works between parties that rely on the CA. You also created and signed a Certificate Signing Request (CSR) for a practice server and then learned how to revoke a certificate. Finally, you learned how to generate and distribute a Certificate Revocation List (CRL) for any system that relies on your CA to ensure that users or servers that should not access services are prevented from doing so. Now you can issue certificates for users and use them with services like OpenVPN. You can also use your CA to configure development and staging web servers with certificates to secure your non-production environments. Using a CA with TLS certificates during development can help ensure that your code and environments match your production environment as closely as possible. If you would like to learn more about how to use OpenSSL, our OpenSSL Essentials: Working with SSL Certificates, Private Keys and CSRs tutorial has lots of additional information to help you become more familiar with OpenSSL fundamentals.","title":"Conclusion"},{"location":"server-management/web-servers/apache/concurrency/","text":"Handling ~4000 Concurrent Connections in Apache By default, Apache web server is configured to support 150 concurrent connections. As your website traffic increases, Apache will start dropping additional requests and this will spoil customer experience. Here\u2019s how to increase max connections in Apache, to support high traffic websites. Install MPM module The MPMs are used to change the basic functionality of the web server. The MPM you use is responsible for the entire HTTP session, starting from listening on the network, taking requests in, and handling those requests. Prefork MPM: Every request gets its own (memory-separated) process. Worker MPM: Multi-threaded Apache, uses threads instead of processes, is generally faster than prefork and might use less memory. Event MPM: threaded like the Worker MPM, but is designed to allow more requests to be served simultaneously by passing off some processing work to supporting threads, freeing up the main threads to work on new requests. We need to install MPM Apache module to be able to serve lots of concurrent connections. You can use mpm_worker or mpm_event module for this, instead of mpm_prefork module which consumes a lot of memory. You can easily install MPM module in Apache with following command CentOS7/RHEL7: adjust /etc/httpd/conf.modules.d/00-mpm.conf Comment the line LoadModule mpm_prefork_module modules/mod_mpm_prefork.so by adding # in front of it Uncomment the line LoadModule mpm_worker_module modules/mod_mpm_worker.so by removing # in front of it. Ubuntu/Debian: Disable the prefork module a2dismod mpm_prefork Enable the worker module a2enmod mpm_worker or enable the event module a2enmod mpm_event Increase Max Connections in Apache Open the configuration files for the apache modules. CentOS/RHEL sudo nano /etc/httpd/conf.modules.d/00-mpm.conf Ubuntu/Debian sudo vi /etc/apache2/mods-available/mpm_worker.conf Adjust the following lines ServerLimit 250 StartServers 10 MinSpareThreads 75 MaxSpareThreads 250 ThreadLimit 64 ThreadsPerChild 32 MaxRequestWorkers 8000 MaxConnectionsPerChild 10000 Restart Apache Server Ubuntu/Debian sudo service apache2 restart CentOS/Redhat/Fedora sudo service httpd restart","title":"Apache"},{"location":"server-management/web-servers/apache/concurrency/#handling-4000-concurrent-connections-in-apache","text":"By default, Apache web server is configured to support 150 concurrent connections. As your website traffic increases, Apache will start dropping additional requests and this will spoil customer experience. Here\u2019s how to increase max connections in Apache, to support high traffic websites.","title":"Handling ~4000 Concurrent Connections in Apache"},{"location":"server-management/web-servers/apache/concurrency/#install-mpm-module","text":"The MPMs are used to change the basic functionality of the web server. The MPM you use is responsible for the entire HTTP session, starting from listening on the network, taking requests in, and handling those requests. Prefork MPM: Every request gets its own (memory-separated) process. Worker MPM: Multi-threaded Apache, uses threads instead of processes, is generally faster than prefork and might use less memory. Event MPM: threaded like the Worker MPM, but is designed to allow more requests to be served simultaneously by passing off some processing work to supporting threads, freeing up the main threads to work on new requests. We need to install MPM Apache module to be able to serve lots of concurrent connections. You can use mpm_worker or mpm_event module for this, instead of mpm_prefork module which consumes a lot of memory. You can easily install MPM module in Apache with following command CentOS7/RHEL7: adjust /etc/httpd/conf.modules.d/00-mpm.conf Comment the line LoadModule mpm_prefork_module modules/mod_mpm_prefork.so by adding # in front of it Uncomment the line LoadModule mpm_worker_module modules/mod_mpm_worker.so by removing # in front of it. Ubuntu/Debian: Disable the prefork module a2dismod mpm_prefork Enable the worker module a2enmod mpm_worker or enable the event module a2enmod mpm_event","title":"Install MPM module"},{"location":"server-management/web-servers/apache/concurrency/#increase-max-connections-in-apache","text":"Open the configuration files for the apache modules. CentOS/RHEL sudo nano /etc/httpd/conf.modules.d/00-mpm.conf Ubuntu/Debian sudo vi /etc/apache2/mods-available/mpm_worker.conf Adjust the following lines ServerLimit 250 StartServers 10 MinSpareThreads 75 MaxSpareThreads 250 ThreadLimit 64 ThreadsPerChild 32 MaxRequestWorkers 8000 MaxConnectionsPerChild 10000","title":"Increase Max Connections in Apache"},{"location":"server-management/web-servers/apache/concurrency/#restart-apache-server","text":"Ubuntu/Debian sudo service apache2 restart CentOS/Redhat/Fedora sudo service httpd restart","title":"Restart Apache Server"}]}